# ✏️ Weekly Paper #10 — NLP 핵심 개념 정리

> 주제: Text Preprocessing · FastText · Attention · Transformer  
> 키워드: `#NLP` `#TextPreprocessing` `#FastText` `#Attention` `#Transformer`

---

## 🇶 1. 텍스트 데이터를 모델에 적용하기 전에 어떤 전처리 과정을 거치나요?

텍스트 데이터는 **그대로는 모델이 이해할 수 없는 비정형 데이터**이기 때문에,  
모델이 계산 가능한 형태로 바꾸는 전처리 과정이 필수적이다.

### 🔹 주요 전처리 단계

1. **정제(Cleaning)**
   - 특수문자, HTML 태그, 이모지 제거
   - 불필요한 공백 정리
   - 대소문자 통일(lowercasing)

2. **토큰화(Tokenization)**
   - 문장을 단어, 형태소, 서브워드 단위로 분리
   - 예: Word / Subword(BPE, SentencePiece) / Character

3. **정규화(Normalization)**
   - 표기 통일 (예: 숫자, 축약형)
   - 어간 추출(Stemming) 또는 표제어 추출(Lemmatization)

4. **불용어 제거(Optional)**
   - 의미 정보가 적은 단어 제거
   - 단, Attention/Transformer 계열에서는 반드시 필요한 단계는 아님

5. **정수 인코딩(Vectorization)**
   - 토큰 → 정수 ID 매핑
   - Padding / Truncation으로 길이 통일

📌 **목표 한 줄 요약**  
> “텍스트를 모델이 연산 가능한 고정된 수치 표현으로 변환하는 것”

---

## 🇶 2. FastText가 Word2Vec과 다른 점은 무엇이며, 어떤 장점이 있나요?

FastText는 **단어를 하나의 토큰으로 보지 않고, 서브워드 단위로 분해**해 학습한다는 점에서  
Word2Vec과 근본적인 차이가 있다.

### 🔹 핵심 차이점

| 구분 | Word2Vec | FastText |
|----|----|----|
| 단어 표현 | 단어 단위 | 서브워드(n-gram) 단위 |
| OOV 처리 | 불가능 | 가능 |
| 희귀 단어 | 취약 | 강함 |
| 형태 정보 | 반영 ❌ | 반영 ⭕ |

### 🔹 FastText의 장점

- **OOV(Out-Of-Vocabulary) 문제 해결**
- 접두사/접미사 등 **형태 정보 반영**
- 한국어처럼 **형태 변화가 많은 언어에 특히 유리**

📌 **한 줄 요약**  
> “FastText는 단어를 ‘조립식’으로 이해한다”

---

## 🇶 3. Attention 메커니즘이 Seq2Seq 모델의 어떤 문제를 해결하나요?

기존 Seq2Seq 모델은 **고정 길이 벡터 하나에 모든 정보를 압축**해야 하는 구조였다.  
이로 인해 문장이 길어질수록 **정보 손실**이 발생하는 문제가 있었다.

### 🔹 기존 Seq2Seq의 문제
- 긴 문장에서 성능 급격히 저하
- 앞부분 정보가 잘 기억되지 않음
- Encoder 출력의 병목 현상

### 🔹 Attention의 해결 방식

- 디코더가 **매 시점마다 인코더의 모든 hidden state를 참조**
- 현재 예측에 **가장 중요한 입력 부분에 가중치 부여**

📌 **핵심 효과**
- 긴 문장 처리 성능 향상
- 문맥 정렬(alignment) 문제 해결
- 번역·요약 품질 대폭 개선

📌 **한 줄 요약**  
> “Attention은 ‘전부 기억’ 대신 ‘지금 중요한 것만 집중’하게 만든다”

---

## 🇶 4. Transformer는 Seq2Seq 구조와 어떤 점에서 근본적으로 다른가요?

Transformer는 **RNN 기반 Seq2Seq 구조를 완전히 버리고**,  
Attention만으로 시퀀스를 처리하는 모델이다.

### 🔹 근본적인 차이

| 항목 | Seq2Seq (RNN) | Transformer |
|----|----|----|
| 순차 처리 | 필수 | ❌ |
| 병렬 처리 | 불가능 | 가능 |
| 핵심 연산 | RNN + Attention | Self-Attention |
| 장기 의존성 | 취약 | 강함 |

### 🔹 Transformer의 핵심 특징

- **Self-Attention**으로 단어 간 관계를 직접 계산
- **Positional Encoding**으로 순서 정보 보완
- 완전 병렬 구조 → 학습 속도 대폭 향상

📌 **패러다임 전환 포인트**
> “기억하면서 읽기 → 한 번에 관계를 계산하기”

📌 **한 줄 요약**  
> “Transformer는 ‘순서대로 읽지 않아도 이해할 수 있다’는 가정을 증명한 모델”

---

## 🧠 전체 요약 한 문장

> **NLP 모델의 발전은 ‘기억의 한계’를 ‘주의(attention)’로 극복해 온 역사다.**

# ✏️ Weekly Paper #11 — Pretrained Language Models & Transformers Ecosystem

> 주제: BERT vs GPT · Hugging Face Transformers · Post-BERT/GPT Models  
> 키워드: `#BERT` `#GPT` `#PretrainedModel` `#Transformers` `#NLP`

---

## 🇶 1. BERT와 GPT의 주요 차이점은 무엇인가요?

BERT와 GPT는 모두 **사전학습(pretraining)** 기반 언어모델이지만,  
**문장을 바라보는 방향성과 목적 자체가 다르다.**

---

### 🔹 기본 구조 & 작동 방식

| 구분 | BERT | GPT |
|---|---|---|
| 구조 | Encoder-only Transformer | Decoder-only Transformer |
| 학습 방향 | 양방향(Bidirectional) | 단방향(Autoregressive) |
| 핵심 아이디어 | 문맥 이해 | 텍스트 생성 |
| 주요 학습 방식 | Masked Language Model (MLM) | Next Token Prediction |

---

### 🔹 BERT (Bidirectional Encoder Representations from Transformers)

- 문장 전체를 **양방향으로 동시에** 읽음
- 일부 토큰을 가리고(masking) 이를 맞히도록 학습
- **문맥 이해에 최적화**

**강점**
- 문장 분류, 감정 분석, QA 등 이해 중심 태스크에 강함
- 입력 전체의 관계를 균형 있게 반영

**적합한 응용**
- 문서 분류
- 질의응답(QA)
- 개체명 인식(NER)
- 자연어 추론(NLI)

---

### 🔹 GPT (Generative Pre-trained Transformer)

- 이전 토큰만 보고 **다음 토큰을 예측**
- 본질적으로 **텍스트 생성 모델**

**강점**
- 자연스러운 문장 생성
- 프롬프트 기반 확장성

**적합한 응용**
- 텍스트 생성
- 요약
- 대화형 AI
- 코드 생성

📌 **핵심 대비 한 줄**
> **BERT는 ‘이해하는 모델’, GPT는 ‘이어 쓰는 모델’**

---

## 🇶 2. Hugging Face Transformers 라이브러리는 무엇인가요?

Hugging Face Transformers는  
**사전학습된 Transformer 기반 모델을 표준화된 인터페이스로 제공하는 오픈소스 라이브러리**다.

---

### 🔹 제공 기능

1. **다양한 사전학습 모델 제공**
   - BERT, GPT, RoBERTa, T5, LLaMA 등 수백 종

2. **Pipeline API**
   - 한 줄로 NLP 태스크 수행
   - 예: text-classification, text-generation, QA 등

3. **Fine-tuning 지원**
   - PyTorch / TensorFlow / JAX 연동
   - Trainer API로 학습 파이프라인 단순화

4. **Tokenizer 통합 관리**
   - 모델별 토크나이저 자동 매칭
   - BPE, WordPiece, SentencePiece 지원

5. **모델 허브(Model Hub)**
   - 수십만 개의 공개 모델 & 데이터셋 공유
   - 실험 재현성과 협업에 강점

📌 **의의**
> “최신 NLP 모델을 연구 코드가 아닌 ‘도구’로 만든 생태계”

---

## 🇶 3. BERT와 GPT 이후 등장한 주요 사전학습 모델과 특징

BERT/GPT 이후의 흐름은 크게 세 방향으로 진화했다.

---

### 🔹 1) BERT 계열의 개선 모델

**RoBERTa**
- BERT 학습 전략 개선
- 더 많은 데이터, 더 긴 학습
- MLM만 사용(NSP 제거)

**ALBERT**
- 파라미터 공유로 모델 경량화
- 메모리 효율 ↑, 성능 유지

**ELECTRA**
- Generator/Discriminator 구조
- “맞혔는가” 대신 “가짜인가” 판별
- 적은 연산으로 높은 성능

---

### 🔹 2) Encoder–Decoder 계열

**T5 (Text-to-Text Transfer Transformer)**
- 모든 NLP 태스크를 “텍스트 → 텍스트”로 통일
- 번역, 요약, QA 등 범용성 뛰어남

**BART**
- 노이즈 복원 기반 사전학습
- 요약·생성 태스크에 강함

---

### 🔹 3) 대규모 생성 모델 (LLM)

**GPT-3 / GPT-4 계열**
- 파라미터 수의 스케일업
- In-context learning 가능

**PaLM / Gemini**
- 초대규모 멀티모달 모델
- 추론 능력 강화

**LLaMA**
- 비교적 작은 파라미터로 높은 성능
- 오픈 웨이트 기반 생태계 확장

---

### 🔹 정리 표

| 계열 | 대표 모델 | 특징 |
|---|---|---|
| BERT 개선 | RoBERTa, ALBERT, ELECTRA | 이해 성능 강화 |
| Enc-Dec | T5, BART | 범용 NLP |
| LLM | GPT-3+, PaLM, LLaMA | 생성·추론 중심 |

📌 **전체 흐름 요약**
> “이해 → 통합 → 추론과 생성”

---

## 🧠 전체 요약 한 문장

> **BERT는 문장을 이해하는 법을, GPT는 언어를 확장하는 법을 가르쳤고,  
그 이후의 모델들은 ‘범용성’과 ‘스케일’로 경쟁하고 있다.**

# Q1. 결정 트리의 장점과 단점은 무엇인가요?

## 1. 결정 트리(Decision Tree)의 장점과 단점
🌳 결정 트리란?

결정 트리는 데이터를 특징(feature) 기준으로 계속 분기해 가면서 예측을 수행하는 모델이다. 트리 구조로 되어 있어서, 최종적으로 “if ~ then ~” 규칙으로 해석할 수 있다.

✅ 장점

해석이 직관적이다 (설명 가능성, Explainability)
트리의 분기만 보면 “왜 이 예측이 나왔는지” 사람이 그대로 이해할 수 있다. 회귀식처럼 복잡한 수식이 아니라 규칙(condition)들의 조합이라서 비전문가에게 설명하기도 쉽다.

전처리가 비교적 단순하다
스케일링(정규화/표준화) 불필요, 범주형 변수도 원-핫 인코딩 없이 처리 가능한 구현들도 있다. → 실무에서 빠르게 시도하기 좋은 베이스라인 모델.

비선형 관계도 잘 잡는다
Feature 간의 복잡한 경계(“이건 30보다 크고 직업이 블루칼라일 때만 YES”) 같은 것도 쉽게 학습한다. 즉, 선형 가정이 필요 없다.

결측치나 이상치에도 비교적 강한 편
어떤 트리 구현(예: CART 기반)들은 결측 분할 처리나 surrogate split 등을 제공해서 결측값에도 어느 정도 견딘다. 이상치도 모델 전체를 망가뜨리는 식은 아니고, 그냥 특정 분기에서 걸러질 수 있다.

분류와 회귀 둘 다 가능하다
범주형 타깃(분류)도 되고, 연속형 타깃(회귀)도 된다. 활용 범위가 넓다.

❌ 단점

과적합(Overfitting)에 매우 취약하다
깊게 자란 트리는 학습 데이터에 특이한 패턴/노이즈까지 외워버린다. 그래서 일반화 성능이 떨어질 수 있다. → 보통 max_depth, min_samples_split 같은 규제를 꼭 건다.

작은 변화에도 결과가 크게 흔들린다 (High Variance)
데이터 샘플이 조금만 바뀌어도 트리 구조 자체가 완전히 달라질 수 있다. 즉, 안정성이 낮다.

복잡한 결정 경계일수록 해석이 다시 어려워진다
얕은 트리는 설명이 쉽지만, 깊고 가지 많은 트리는 사실상 사람이 다 읽기 힘든 규칙덩어리가 된다. “설명 가능성”은 트리가 얼마나 단순하냐에 따라 유효하다.

단일 결정 트리는 SOTA 모델 대비 성능이 제한적이다
순수한 1개의 결정 트리는 복잡한 데이터(고차원, 상호작용 많은 데이터)에서 성능이 부족할 수 있다. 그래서 현실에서는 보통 여러 트리를 묶은 앙상블(랜덤 포레스트, 부스팅 등)을 더 많이 쓴다.


# Q2. 부스팅은 어떤 특징을 가진 앙상블 기법인가요?

## 2. 부스팅(Boosting)
⚡ 부스팅이란?

부스팅은 약한 학습기(weak learner)를 여러 개 순차적으로 학습시키고, 그 예측을 합쳐서 강한 학습기(strong learner)를 만드는 앙상블 기법이다.
핵심 포인트는 “순차적으로 약점을 보완한다” 는 것.

첫 번째 모델이 틀린 부분(예: 잘못 분류된 샘플)을 다음 모델에서 더 집중해서 학습한다.

이렇게 점점 실수를 줄여 나가며 최종적으로 강력한 모델을 만든다.
부스팅은 병렬로 여러 트리를 한 번에 학습하는 랜덤 포레스트(=배깅)와 달리, 순차적으로 업데이트한다는 점이 특징이다. 
Analytics Vidhya

또한 부스팅은 단일 트리보다 일반적으로 훨씬 더 높은 예측 성능을 낸다. 


* 수업에서 다룬 AdaBoost 요약

매 단계에서 “이전까지 틀렸던 샘플”에 더 큰 가중치를 부여해서, 다음 약한 분류기(보통 얕은 결정 트리 = decision stump)가 그 샘플을 더 잘 맞추도록 훈련한다.

최종 예측은 여러 약한 분류기의 가중치 있는 투표(분류) or 합(회귀)로 결정된다.

장점: 간단하고, 비교적 해석 가능하고, 적절한 하이퍼파라미터면 꽤 튼튼한 성능.

단점: 노이즈나 이상치에 예민하다. 왜냐면 “틀린 애들에 가중치 더 주기” 전략이 아웃라이어에도 집착하게 만들 수 있기 때문. 


## AdaBoost 말고, 널리 쓰이는 부스팅 모델들

아래는 실무/대회(캐글, 금융 리스크 모델링 등)에서 메이저인 애들이다. 전부 “Gradient Boosting 계열”로 묶이는 애들이고, 전반적으로 트리 기반이다. 


1) Gradient Boosting (GBM / GradientBoostingMachine)

아이디어

현재 모델의 오차(잔차)를 “기울기(gradient)” 방향으로 줄이는 새로운 약한 학습기(대개 결정 트리)를 계속 추가해 나간다.

즉, “손실함수의 기울기를 따라가면서 한 트리씩 개선”하는 방식이라서 이름이 Gradient Boosting.

**장점**

다양한 손실 함수(분류의 로지스틱 손실, 회귀의 MSE 등)에 맞춰 유연하게 최적화할 수 있다.

비교적 높은 예측 성능.

**단점**

트리를 순차적으로 하나씩 쌓으므로 학습이 느릴 수 있다.

하이퍼파라미터(학습률, 트리 깊이 등)를 잘못 잡으면 과적합되거나, 반대로 언더핏된다.

대용량 데이터셋에서 기본 구현(sklearn GradientBoostingClassifier/Regressor)은 느리고 무거울 수 있다. 


2) XGBoost (Extreme Gradient Boosting)

아이디어 / 특징

Gradient Boosting을 엔지니어링적으로 강화한 버전.

정규화(L1/L2 penalty)를 트리 분할에 직접 넣어서 복잡한 모델을 억제하고, 일반화 성능을 끌어올린다.

병렬화, 분산 학습, 결측치 자동 처리, 캐시 최적화 등 “실전에서 빠르고 안정적으로 돌아가게” 개선된 구현이다. 


**장점**

높은 예측력: 캐글 대회 상위권 단골.

정규화가 있어서 과적합을 비교적 잘 통제한다.

결측치를 내부적으로 다루는 로직이 있어 전처리 부담을 줄인다.

CPU/GPU 모두 지원하며 대규모 데이터에도 잘 버틴다. 


**단점**

하이퍼파라미터가 많다. (learning_rate, max_depth, subsample, colsample_bytree, reg_alpha 등…)

그만큼 튜닝이 까다롭다.

기본적으로 트리를 level-wise(깊이 균형 맞추며) 확장해서 비교적 안정적이지만, 여전히 깊은 트리는 과적합 위험이 있다. 


3) LightGBM (Light Gradient Boosting Machine)

아이디어 / 특징

마이크로소프트가 만든 고성능 Gradient Boosting 계열 모델.

연속형 변수를 “히스토그램 bin”으로 압축해서 학습하기 때문에 메모리 사용량이 훨씬 적고, 매우 빠르다.

트리를 level-wise가 아니라 leaf-wise (loss 감소가 가장 큰 리프부터 계속 확장) 방식으로 키운다. 이건 손실을 더 빨리 줄일 수 있어서 고성능을 내는 핵심 포인트다. 


**장점**

압도적으로 빠른 학습 속도, 낮은 메모리 사용량. 대용량 / 고차원의 데이터에서 특히 강하다.

보통 XGBoost랑 비슷하거나 그 이상인 정확도를 뽑는 경우가 많다.

GPU 지원, 분산 학습 지원. 


**단점**

leaf-wise 성장은 데이터가 적을 때 과적합(overfitting)으로 이어질 수 있다. (너무 공격적으로 특정 영역에만 깊어지는 나무)

카테고리형 특성도 지원하긴 하지만, 잘못 다루면 여전히 과적합/데이터 누수(leakage) 위험이 있다.

작은/깨끗하지 않은 데이터셋에서 오히려 XGBoost보다 불안정할 수 있다. 


4) CatBoost

아이디어 / 특징

Yandex가 개발. 이름처럼 Categorical + Boosting.

범주형 변수(문자열, 고유 ID 등)를 원-핫 인코딩 없이도 내부에서 타깃 기반 통계 인코딩을 안전하게 처리해준다.

순서를 고려한 통계 인코딩 등 편향을 줄이는 기법이 내장돼 있어서, 범주형 변수가 많은 데이터에서 특히 강력하다. 
AI Powered Product Engineering

**장점**

범주형 처리 자동: 원-핫 인코딩 지옥에서 탈출. 전처리 부담이 확 줄어든다.

튜닝 없이도 높은 기본 성능(디폴트 파라미터가 강하다).

과적합을 줄이는 여러 규제 테크닉이 내장돼 있어 안정적으로 학습된다.

**단점**

내부 로직이 복잡해서 “왜 이렇게 예측했는지”를 단순 트리보다 설명하기 어렵다. (해석 가능성 ↓)

여전히 부스팅 계열이라 학습은 순차적이고 무겁다. 초거대 데이터에서 극한의 속도가 필요하면 LightGBM이 더 빠를 수 있다. 
AI Powered Product Engineering

정리하자면

부스팅 = “앞 모델의 실수를 다음 모델이 때려잡는” 순차적 앙상블.

AdaBoost = 잘못 맞춘 샘플에 가중치 ↑. 단순하고 클래식하지만 노이즈에 약함. 

Gradient Boosting = 손실 함수의 기울기를 따라가며 잔차를 줄이는 방향으로 트리를 계속 추가. 유연하지만 느릴 수 있음. 

XGBoost = Gradient Boosting을 정규화+최적화로 강화. 빠르고 강력하고 산업 표준. 

LightGBM = 히스토그램/leaf-wise 전략으로 초고속, 저메모리, 대규모 데이터에 특히 강함. 단, 과적합 주의. 

CatBoost = 범주형 변수 자동 처리의 제왕. 적은 전처리로도 강력. 다만 설명 가능성은 떨어질 수 있음. 
AI Powered Product Engineering


# Q3. 차원 축소 기법인 주성분 분석과 요인 분석의 차이는 무엇인지 설명해 주세요.

## 3. 차원 축소: 주성분 분석(PCA) vs 요인 분석(FA)
먼저: 차원 축소란?

특징(변수)이 너무 많으면 모델은 복잡해지고, 노이즈도 많고, 시각화도 어렵다.
차원 축소는 “정보는 최대한 유지하면서 변수 수를 줄이는” 기술.

PCA랑 요인분석은 겉으로 비슷해 보이지만, 목적과 해석이 다르다.

🌀 PCA (Principal Component Analysis, 주성분 분석)

목적

“데이터의 분산을 최대한 보존하면서, 축을 새로 만든다.”

즉, 데이터를 가장 잘 퍼뜨리는(variance가 큰) 새로운 직교 좌표계(주성분)를 찾는다.

어떻게 작동하나

공분산 행렬의 고유값 분해 / SVD로 주성분(Principal Components)을 구한다.

첫 번째 주성분 PC1: 데이터가 가장 넓게 퍼지는 방향

두 번째 주성분 PC2: 그다음으로 많이 퍼지는 방향 (PC1과 직교)

이렇게 얻은 주성분 축들에 원 데이터를 투영하면, 차원이 줄어든다.

해석 포인트

PCA의 새 축(주성분)은 “순수하게 분산을 최대화하는 방향”일 뿐, 그 축이 어떤 ‘숨은 의미 있는 요인’을 꼭 대표한다고 보장되지 않는다.

즉, “설명 가능한 숨은 원인”이라기보다 “정보(분산)를 많이 들고 있는 축”이다.

언제 쓰나

고차원 데이터를 저차원으로 시각화 (예: 100차원 → 2차원)

모델에 넣기 전 노이즈/중복 정보 줄이기

연속형 변수 위주일 때 특히 많이 쓴다.

🌌 요인 분석 (Factor Analysis)

목적

“관측된 여러 변수들은 사실 공통된 숨은 잠재 요인(latent factor)들 때문에 나타나는 게 아닐까?”

즉, 데이터 뒤에 있는 ‘보이지 않는 심리/구조적 요인’을 찾아내고 싶을 때 쓴다.

어떻게 작동하나

각 관측 변수 = (몇 개의 공통 요인들의 선형결합) + (그 변수 고유의 오차/특수 요인)

모델이 “관측 변수들 사이의 공분산 구조를 잘 설명하는 소수의 잠재 요인(Latent factors)”을 추정한다.

여기서 그 잠재 요인들은 해석 가능한 이름을 붙일 수 있다고 가정한다. 예:

요인1: “재무 건전성”

요인2: “마케팅 반응성”

요인3: “이탈 위험 성향”
이런 식으로 비즈니스/심리적 의미를 부여하는 게 목적 자체다.

해석 포인트

요인분석은 “설명 가능한 숨은 원인”을 찾고자 하는 통계적/심리측정적 모델이다.

변수들 간 상관관계를 생성하는 공통 요인을 복원하려 한다.

오차항(고유 요인, unique factor)도 명시적으로 모델링한다.

PCA vs 요인분석 한 방 비교
비교 항목	PCA	요인 분석 (FA)
최종 목표	분산 보존 / 차원 축소	잠재 요인(숨은 원인) 해석
생각방식	“정보를 많이 담는 축으로 돌려서 압축하자”	“이 변수들, 사실 같은 숨은 애(요인) 때문에 움직이는 거 아니야?”
수학적 초점	공분산/분산을 최대화하는 직교 주성분	상관구조를 설명하는 소수의 공통 인자 + 고유오차
결과 해석	주성분은 해석적으로 의미 없을 수도 있음	요인은 해석 가능한 레이블(태그)을 붙이는 게 핵심
사용 사례	시각화, 차원 축소 전처리, 노이즈 제거	심리검사, 고객 세분화에서의 잠재 성향 추정, 경제 지표의 공통 요인 분석

요약하면:

PCA는 “데이터를 압축해서 잘 보여주고 싶을 때” 쓰는 수학적 변환.

요인분석은 “이 변수들이 왜 이렇게 같이 움직이는지, 숨은 요인은 뭔지”를 알고 싶을 때 쓰는 통계 모델.

마지막 한 줄 요약 🪄

결정 트리: 이해하기 쉽고 빠르지만, 혼자 두면 과적합 괴물.

부스팅: “실수한 부분 집중 공략”을 반복해서, 약한 모델들을 강하게 만든다. AdaBoost / Gradient Boosting / XGBoost / LightGBM / CatBoost 등 각자 속도·해석력·전처리 편의성에서 개성이 다르다. 

PCA vs 요인분석: PCA는 데이터 압축용, 요인분석은 숨은 요인 해석용.

# ✏️ Weekly Paper #12 — LLM 신뢰성·확장성·효율성 이슈

> 키워드: `#Hallucination` `#ScalingLaw` `#LLM` `#PEFT` `#FineTuning`

---

## 🇶 1. LLM의 할루시네이션(Hallucination)이란 무엇이며, 왜 문제가 되나요?

### 🔹 할루시네이션이란?

**할루시네이션(Hallucination)**이란  
LLM이 **사실처럼 보이지만 실제로는 근거 없거나 틀린 정보를 생성하는 현상**을 말한다.

- 존재하지 않는 논문·법·인물·통계 수치 생성
- 질문의 맥락을 벗어난 그럴듯한 답변
- 출처를 묻지 않았는데도 확신에 찬 어조로 오류 전달

> Why?   
> LLM은 “사실을 검색”하는 모델이 아니라  
> **“다음에 올 토큰의 확률을 예측하는 모델”이기 때문**

---

### 🔹 왜 문제가 되나?

1. **신뢰성 붕괴**
   - 의료, 법률, 금융 등 고위험 도메인에서 치명적
2. **검증 비용 증가**
   - 사람이 매번 팩트체크 필요
3. **자동화 한계**
   - 단독 사용 시 업무 리스크 발생

---

### 🔹 업계의 주요 대응 전략

#### 1️⃣ RAG (Retrieval-Augmented Generation)
- 외부 문서 검색 후, **근거 기반 생성**
- “모르는 건 모른다”를 가능하게 함

#### 2️⃣ Instruction / RLHF 강화
- “추측하지 말고, 근거가 없으면 말하지 말라”는 학습
- 응답 톤과 안전성 개선

#### 3️⃣ Tool Calling / Search 연동
- 계산·검색은 모델이 직접 하지 않고 **도구에 위임**
- 최신 정보·정확도 보완

#### 4️⃣ Self-Check / Verification
- 모델이 **자기 응답을 다시 검토**
- 다중 샘플링 후 합의 방식 사용


---

## 🇶 2. 모델 크기만 키우면 성능이 왜 어느 순간 둔화될까요?

### 🔹 스케일링의 한계

모델 파라미터 수를 늘리면 초기에는 성능이 개선되지만,  
**일정 지점 이후에는 증가 대비 성능 향상이 급격히 둔화**된다.

---

### 🔹 주요 이유

1. **데이터 품질 한계**
   - 모델은 커졌는데 학습 데이터는 중복·노이즈 증가
   - “더 많이”보다 “더 좋은 데이터”가 중요해짐

2. **학습 효율 저하**
   - 파라미터 증가 → 최적화 난이도 증가
   - 수렴 속도 저하 및 불안정성

3. **추론 비용 폭증**
   - 메모리·지연 시간·전력 소모 급증
   - 실서비스 적용에 제약

4. **능력 포화**
   - 단순 언어 패턴 학습은 이미 포화
   - 추론·계획·세계 이해는 구조적 개선 필요

> (업계 인식 변화)  
> “**Bigger is better**” → “**Smarter is better**”

---

## 🇶 3. PEFT는 왜 필요하며, 언제 특히 효과적인가요?

### 🔹 PEFT(Parameter-Efficient Fine-Tuning)란?

**PEFT**는  
모델 전체를 학습하지 않고, **일부 파라미터만 효율적으로 조정**하는 미세조정 기법이다.

---

### 🔹 왜 필요한가?

1. **자원 절약**
   - 전체 파인튜닝 대비 GPU·메모리 사용량 대폭 감소
2. **빠른 실험**
   - 작은 데이터로도 효과적인 적응 가능
3. **원본 모델 보존**
   - 기본 성능 유지 + 태스크 특화

---

### 🔹 대표적인 PEFT 방식

- **LoRA**: 저차원 행렬을 추가해 업데이트
- **Adapter**: 중간 레이어에 작은 모듈 삽입
- **Prefix / Prompt Tuning**: 입력 프롬프트 학습

---

### 🔹 특히 효과적인 상황

- 도메인 특화 모델 (법률, 의료, 사내 문서)
- 사내 배포 / 온프레미스 환경
- 다수 태스크를 하나의 베이스 모델로 운영할 때
- 빠른 PoC → 서비스 전환 과정

> 즉, **PEFT는 ‘큰 모델을 작게 쓰는 기술’**

---

## 전체 요약 한 문장

> **LLM의 발전은  
> ‘더 크게’에서 ‘더 정확하게, 더 효율적으로’로 이동하고 있다.**

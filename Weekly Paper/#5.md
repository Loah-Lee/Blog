# 딥러닝 프레임워크 비교 및 PyTorch 텐서 개념 정리

---

## 1️⃣ PyTorch vs TensorFlow 비교

### 개요
PyTorch와 TensorFlow는 현재 가장 널리 사용되는 두 개의 대표적인 딥러닝 프레임워크이다.  
두 라이브러리 모두 GPU 가속 연산, 자동 미분(autograd), 다양한 신경망 계층 구현 등을 지원하지만  
철학과 사용 방식에서 분명한 차이를 보인다.

---

### 1.1 개발 및 철학의 차이

| 항목 | PyTorch | TensorFlow |
|------|----------|-------------|
| 개발 주체 | Meta (Facebook) | Google |
| 출시 시기 | 2016년 | 2015년 |
| 철학 | **Pythonic**하고 직관적인 코드 흐름 | **대규모 배포 및 프로덕션 친화적** 설계 |
| 그래프 실행 | 즉시 실행(Eager Execution) 중심 | 초기엔 Static Graph 중심 → 2.x부터 Eager 지원 |
| 디버깅 방식 | 일반 파이썬처럼 디버깅 가능 | 그래프 기반으로 디버깅 상대적으로 복잡 |

---

### 1.2 코드 스타일 비교

**PyTorch**
```python
import torch
import torch.nn as nn

x = torch.randn(3, 4)
layer = nn.Linear(4, 2)
y = layer(x)
print(y)
```

→ 실행 즉시 결과가 출력되는 **Eager Execution** 방식.  
직관적이며, 파이썬 코드처럼 흐름을 따라가며 실험하기 쉽다.

**TensorFlow**
```python
import tensorflow as tf

x = tf.random.normal((3, 4))
layer = tf.keras.layers.Dense(2)
y = layer(x)
print(y)
```

TensorFlow 2.x부터는 PyTorch처럼 즉시 실행이 가능하지만,  
내부적으로는 여전히 **그래프 기반 최적화**(Graph + Eager Hybrid)가 가능해  
대규모 서비스 배포 시 유리하다.

---

### 1.3 생태계와 활용도

| 비교 항목 | PyTorch | TensorFlow |
|------------|----------|-------------|
| 연구 분야 | 연구자·논문에서 가장 많이 사용 | 여전히 산업계에서 강세 |
| 배포 지원 | TorchServe, ONNX 등 | TensorFlow Serving, TFLite, TPU 등 |
| 커뮤니티 | 빠르게 성장 중, 연구용 오픈소스 많음 | 생태계가 넓고, 상용 도구와 통합 용이 |
| 학습 곡선 | 쉬움 (Python 코드와 유사) | 다소 가파름 (구조적 설계 필요) |

---

### 1.4 요약

- **PyTorch** → 실험과 프로토타이핑에 강함. “직관적이고 코딩이 즐겁다.”  
- **TensorFlow** → 대규모 서비스 배포에 강함. “산업적 안정성과 최적화 중심.”  
- 현재는 두 프레임워크 모두 기능이 많이 수렴했고,  
  사용 목적(연구 vs 프로덕션)에 따라 선택이 갈리는 편이다.

---

## 2️⃣ PyTorch의 Tensor와 NumPy Array의 차이

### 2.1 Tensor란?
PyTorch의 **Tensor**는 수학에서 말하는 다차원 배열(Multi-dimensional Array)이다.  
NumPy의 `ndarray`와 매우 유사하게 동작하지만,  
**GPU 연산과 자동 미분**을 지원한다는 점이 핵심적인 차이이다.

---

### 2.2 공통점
- 형태(Shape)와 자료형(Dtype)을 가진 다차원 배열 구조  
- 인덱싱, 슬라이싱, 브로드캐스팅 등의 연산 지원  
- NumPy와 자유로운 상호 변환 가능 (`torch.from_numpy`, `tensor.numpy()`)

---

### 2.3 차이점 정리

| 구분 | PyTorch Tensor | NumPy Array |
|------|----------------|--------------|
| 연산 장치 | CPU, GPU 모두 가능 | CPU 전용 |
| 자동 미분 | 지원 (`requires_grad=True`) | 미지원 |
| 딥러닝 학습용 | 네트워크 파라미터 업데이트에 사용 | 불가능 |
| 생성 방식 | `torch.tensor()`, `torch.randn()` 등 | `np.array()`, `np.random.randn()` 등 |
| 상호 변환 | `.numpy()`, `torch.from_numpy()` | 가능 (메모리 공유) |

---

### 2.4 예시 코드

```python
import numpy as np
import torch

# NumPy 배열 생성
arr = np.array([[1, 2], [3, 4]], dtype=np.float32)

# PyTorch 텐서로 변환
tensor = torch.from_numpy(arr)

# GPU로 옮기기
tensor_gpu = tensor.to('cuda')

print(tensor)
print(tensor_gpu)
```

→ 동일한 데이터를 GPU에서 연산할 수 있으며,  
필요시 자동 미분 기능을 켜서 모델 학습에 직접 활용할 수 있다.

---

### 2.5 한 줄 요약

> NumPy의 Array가 **CPU에서 수치 계산용 도구**라면,  
> PyTorch의 Tensor는 **GPU에서도 동작하며,  
> 자동 미분이 가능한 딥러닝 전용 데이터 구조**이다.

---

### 결론

- 연구와 실험 중심이라면 **PyTorch**  
- 대규모 서비스 및 모바일 배포 중심이라면 **TensorFlow**  
- 그리고 두 프레임워크 모두 Tensor를 기반으로,  
  복잡한 신경망 연산을 효율적으로 수행한다.

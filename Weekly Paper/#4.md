# 딥러닝 & 머신러닝 핵심 개념 정리


## 🇶 딥러닝과 머신러닝 간의 포함관계에 대해 설명해주세요.

### 1️⃣ 포함 관계 개요

AI (인공지능)
└── Machine Learning (기계학습)
└── Deep Learning (심층학습)


- **머신러닝(Machine Learning)**: 데이터를 통해 규칙을 학습하는 AI의 하위 분야  
- **딥러닝(Deep Learning)**: 머신러닝 중에서도 **인공신경망(Neural Network)** 을 심층 구조로 확장한 방식  


### 2️⃣ 핵심 차이: "특징(feature)을 누가 만드는가"

| 항목 | 머신러닝 | 딥러닝 |
|------|-----------|--------|
| 특징 추출 | 사람이 직접 설계 | 모델이 스스로 학습 |
| 구조 복잡도 | 단순 (SVM, Tree 등) | 깊은 신경망 (CNN, RNN, Transformer 등) |
| 데이터 요구량 | 적음 | 매우 많음 |
| 계산 자원 | CPU로 충분 | GPU, TPU 필요 |


### 3️⃣ 실무 관점에서의 구분

| 구분 | 머신러닝 | 딥러닝 |
|------|-----------|--------|
| 데이터 형태 | 정형 (표, 수치) | 비정형 (이미지, 오디오, 텍스트 등) |
| 대표 모델 | XGBoost, SVM, Random Forest | CNN, RNN, Transformer |
| 주로 쓰이는 분야 | 예측, 분류, 이상탐지 | 이미지·음성 인식, 번역, 생성 모델 |


### 4️⃣ 요약

> **딥러닝은 머신러닝의 한 갈래이지만,  
> '특징을 스스로 학습한다'는 점에서 머신러닝과 다르다.**

---

## 🇶 딥러닝의 성능 향상을 위해 고려하는 하이퍼파라미터의 종류에는 어떤 것들이 있는지 설명해주세요.

### 1️⃣ 하이퍼파라미터란?

> 모델이 **스스로 학습하지 못하는 외부 설정값**,  
> 즉 **‘학습하는 방법’을 사람이 정하는 변수**다.

예:  
- `learning_rate` → 공부 속도  
- `epochs` → 공부 기간  
- `batch_size` → 한 번에 외우는 양  


### 2️⃣ 주요 하이퍼파라미터 분류

| 구분 | 항목 | 설명 |
|------|------|------|
| 학습 관련 | `learning_rate`, `batch_size`, `epochs` | 학습 속도와 안정성 |
| 모델 구조 | `num_layers`, `neurons`, `activation` | 모델 복잡도와 표현력 |
| 규제 관련 | `dropout`, `weight_decay`, `batch_norm` | 과적합 방지, 일반화 |
| 최적화 관련 | `optimizer` (SGD, Adam 등) | 파라미터 업데이트 방식 |
| 데이터 증강 | `augmentation`, `noise_level` | 데이터 다양성 확보 |
| 스케줄링 | `lr_scheduler`, `warmup_steps` | 학습률 변화 곡선 조절 |


### 3️⃣ 하이퍼파라미터 간의 상호작용

- `batch_size` ↑ → 학습 안정성 ↑, 하지만 일반화 ↓  
- `learning_rate` ↑ → 빠른 수렴, 그러나 발산 위험 ↑  
- `dropout` ↑ → 과적합 ↓, 하지만 학습 속도 ↓  

> 좋은 성능은 **하나의 값이 아니라, 균형의 조합**에서 나온다.  


### 4️⃣ 탐색 및 최적화 방법

- **Grid Search**: 완전탐색 (비효율적이지만 직관적)  
- **Random Search**: 효율적인 무작위 탐색  
- **Bayesian Optimization / Hyperband / Optuna**: 지능적 탐색  
- **AutoML / NAS**: 모델이 스스로 최적 하이퍼파라미터를 탐색  


### 5️⃣ 요약

> **딥러닝의 성능은 네트워크 깊이보다,  
> 하이퍼파라미터의 ‘균형 잡힌 조율’에 달려 있다.**
